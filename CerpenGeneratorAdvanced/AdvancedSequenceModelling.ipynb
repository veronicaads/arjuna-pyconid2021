{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AdvancedSequenceModelling.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ad0879ff197c42028aa5ade7d684f78d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3ba5b5057e3049068c87473fda3eee73","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0cc37d6cf2d046a498d34379d8a83a40","IPY_MODEL_9c33bb67f2564385a2f0b5cc35874495","IPY_MODEL_aa1a2ab1ad9b4636b101afdffccb3331"]}},"3ba5b5057e3049068c87473fda3eee73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0cc37d6cf2d046a498d34379d8a83a40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0fc01b9a5abe4526a2cc59b6f647cf25","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"training routine: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_18516a2634d9460c93fdb8385153a8d3"}},"9c33bb67f2564385a2f0b5cc35874495":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c603965987ed4a5c8680f51906944ff3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":100,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":100,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_419fd2dad412406aaf07ae5e66ab5040"}},"aa1a2ab1ad9b4636b101afdffccb3331":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_25b9173f814e4ef394a63146686643d9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 100/100 [07:19&lt;00:00,  4.39s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4fc991644b1b4c62bb51028334d1487e"}},"0fc01b9a5abe4526a2cc59b6f647cf25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"18516a2634d9460c93fdb8385153a8d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c603965987ed4a5c8680f51906944ff3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"419fd2dad412406aaf07ae5e66ab5040":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"25b9173f814e4ef394a63146686643d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4fc991644b1b4c62bb51028334d1487e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4839d25e9ca740e5b0f412254f7ef0bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bcb6d0f0ce4d4281bb38f9cc1a0f4476","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f3592c0725a54f9e884405749bd0d3a6","IPY_MODEL_3cee14fff0114e28ba9b2ffcc693af86","IPY_MODEL_3da162dc2802497791dc88130b7e2db5"]}},"bcb6d0f0ce4d4281bb38f9cc1a0f4476":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f3592c0725a54f9e884405749bd0d3a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f37ee68cc122431da860d247b22afe84","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"split=train:  89%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4cda0ae7abad463c8435edec649ee6ea"}},"3cee14fff0114e28ba9b2ffcc693af86":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_70196b79ccc74515812cf4d3aa711422","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":9,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":8,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c49b3577ab2845d89c1458db89c570e4"}},"3da162dc2802497791dc88130b7e2db5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_39cfd7507ef44699bd652e97c4706931","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8/9 [07:18&lt;00:00,  2.12it/s, acc=99.7, epoch=99, loss=0.0718]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_810deb7bb3c446129d03d5af257047a9"}},"f37ee68cc122431da860d247b22afe84":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4cda0ae7abad463c8435edec649ee6ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70196b79ccc74515812cf4d3aa711422":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c49b3577ab2845d89c1458db89c570e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"39cfd7507ef44699bd652e97c4706931":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"810deb7bb3c446129d03d5af257047a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"911f2b2728d640e197df88c49109930b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_78c795fd57af4f03abfdee1312a0e5b5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fd5da426e2e34625a030b15ee39c33c5","IPY_MODEL_02cdaae8149149c7ab9d72c582ae5f7f","IPY_MODEL_73738692503343d2a6ac91cdf67ff5cb"]}},"78c795fd57af4f03abfdee1312a0e5b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fd5da426e2e34625a030b15ee39c33c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5005edc0777a43eabfaad2dd592ec21b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"split=val:   0%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8ae1531c95504611997a18ac67ff8865"}},"02cdaae8149149c7ab9d72c582ae5f7f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_702cb905c553493e81c8d8df92320d9f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_516d8fb5df494995aa0a7335433b20c5"}},"73738692503343d2a6ac91cdf67ff5cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f656e8c49d1c48eea91df574bb8bdbb2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/1 [07:18&lt;00:04,  4.53s/it, acc=13.2, epoch=99, loss=10.9]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ec2caf03d9564df7bd3f5864fe785f4e"}},"5005edc0777a43eabfaad2dd592ec21b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8ae1531c95504611997a18ac67ff8865":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"702cb905c553493e81c8d8df92320d9f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"516d8fb5df494995aa0a7335433b20c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f656e8c49d1c48eea91df574bb8bdbb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ec2caf03d9564df7bd3f5864fe785f4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"OfdW2mKNtN12"},"source":["# Arjuna: Generate Indonesian Poets and Poems using python base NLP model"]},{"cell_type":"code","metadata":{"id":"dd7AnCfPpYo4"},"source":["import numpy as np\n","from collections import Counter\n","import string\n","import torch\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from argparse import Namespace\n","import torch.optim as optim\n","from tqdm.notebook import tqdm\n","import torch.nn.functional as F\n","import re\n","import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AvU7eaNw2m-T","executionInfo":{"status":"ok","timestamp":1637336427643,"user_tz":-420,"elapsed":43402,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"b4796352-78fc-4df4-b767-8a13a78d3cb0"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"0Wm4HQCC2-I7"},"source":["## Dataset Build up\n"]},{"cell_type":"code","metadata":{"id":"QfEZMrMt2C7A"},"source":["poet_df = pd.read_csv(\"/content/drive/MyDrive/Pycon2021/Dataset Puisi - Manual - Sheet1.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"tiVgnuNV2C7B","executionInfo":{"status":"ok","timestamp":1637336510519,"user_tz":-420,"elapsed":336,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"e954c9a8-10dc-4d9b-ffe1-1261ba78da91"},"source":["poet_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>content</th>\n","      <th>poem name</th>\n","      <th>age</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ramadhan K H</td>\n","      <td>Seruling berkawan pantun,\\nTangiskan derita or...</td>\n","      <td>Priangan Si Jelita</td>\n","      <td>1956.0</td>\n","      <td>keindahan alam, gadis-gadis</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Chairil Anwar</td>\n","      <td>Kalau sampai waktuku\\nAku mau tak seorang kan ...</td>\n","      <td>Aku</td>\n","      <td>1943.0</td>\n","      <td>individualistic, nature, and vitality</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Chairil Anwar</td>\n","      <td>Di masa pembangunan ini\\r\\nTuan hidup kembali\\...</td>\n","      <td>Diponegoro</td>\n","      <td>1943.0</td>\n","      <td>perjuangan</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Chairil Anwar</td>\n","      <td>Kepada pemeluk teguh\\r\\n\\r\\nTuhanku\\r\\nDalam t...</td>\n","      <td>Doa</td>\n","      <td>1943.0</td>\n","      <td>agama</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Chairil Anwar</td>\n","      <td>Ini muka penuh luka\\r\\nSiapa punya?\\r\\n\\r\\nKu ...</td>\n","      <td>Aku Berkaca</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          author  ...                                   type\n","0   Ramadhan K H  ...            keindahan alam, gadis-gadis\n","1  Chairil Anwar  ...  individualistic, nature, and vitality\n","2  Chairil Anwar  ...                             perjuangan\n","3  Chairil Anwar  ...                                  agama\n","4  Chairil Anwar  ...                                    NaN\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9LsyvfDs2C7B","executionInfo":{"status":"ok","timestamp":1637336514697,"user_tz":-420,"elapsed":316,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"8f965eb9-abc7-4cf4-8331-8bf2a91e3cc5"},"source":["poet_df.shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["162"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XOb3zhH32C7C","executionInfo":{"status":"ok","timestamp":1637336516583,"user_tz":-420,"elapsed":7,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"3f14512f-3f00-4484-bed2-8950ae2bc1bd"},"source":["poet_df['content'][0].split(\"\\n\\n\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Seruling berkawan pantun,\\nTangiskan derita orang priangan,\\nSelendang merah, merah darah\\nMenurun di Cikapundung.',\n"," 'Bandung, dasar di danau\\nLari bertumpuk di bukit-bukit.',\n"," 'Seruling menyendiri di tepi-tepi\\nTangiskan keris hilang di sumur\\nMelati putih, putih hati,\\nHilang kekasih dikata gugur.',\n"," 'Bandung, dasar di danau\\nDerita memantul di kulit-kulit.']"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"ozywzioh2C7C","executionInfo":{"status":"ok","timestamp":1637336519535,"user_tz":-420,"elapsed":311,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"39a053d9-f83f-47ce-aad7-0ec13ba3f97c"},"source":["poet_df['content'][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Seruling berkawan pantun,\\nTangiskan derita orang priangan,\\nSelendang merah, merah darah\\nMenurun di Cikapundung.\\n\\nBandung, dasar di danau\\nLari bertumpuk di bukit-bukit.\\n\\nSeruling menyendiri di tepi-tepi\\nTangiskan keris hilang di sumur\\nMelati putih, putih hati,\\nHilang kekasih dikata gugur.\\n\\nBandung, dasar di danau\\nDerita memantul di kulit-kulit.'"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"CQB8es0I3ogt"},"source":["### import NLTK and Tokenizing"]},{"cell_type":"code","metadata":{"id":"1Cp8wE8X3bx9"},"source":["poet_dataset = {'first': [], 'second': []}\n","\n","num_poets = poet_df.shape[0]\n","\n","for i in range(num_poets):\n","    sentences = poet_df['content'][i].split(\"\\n\\n\")\n","    sentences_len = len(sentences)\n","    WINDOW_SIZE = 2\n","    for index in range(sentences_len - WINDOW_SIZE):\n","        prev_sentence = sentences[index]\n","        next_sentence = sentences[index + 1]\n","\n","        # print(\"1 \" + prev_sentence)\n","        # print(\"2 \" + next_sentence)\n","        poet_dataset['first'].append(prev_sentence)\n","        poet_dataset['second'].append(next_sentence)\n","\n","poet_dataset = pd.DataFrame.from_dict(poet_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"NhUagv32CuYn","executionInfo":{"status":"ok","timestamp":1637336537132,"user_tz":-420,"elapsed":351,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"6ce33463-6bb3-489d-c3a9-20b1217fec86"},"source":["poet_dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>first</th>\n","      <th>second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Seruling berkawan pantun,\\nTangiskan derita or...</td>\n","      <td>Bandung, dasar di danau\\nLari bertumpuk di buk...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Bandung, dasar di danau\\nLari bertumpuk di buk...</td>\n","      <td>Seruling menyendiri di tepi-tepi\\nTangiskan ke...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Kalau sampai waktuku\\nAku mau tak seorang kan ...</td>\n","      <td>Aku ini binatang jalang\\nDari kumpulannya terb...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Aku ini binatang jalang\\nDari kumpulannya terb...</td>\n","      <td>Biar peluru menembus kulitku\\nAku tetap merada...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Biar peluru menembus kulitku\\nAku tetap merada...</td>\n","      <td>Luka dan bisa kubawa berlari\\nBerlari\\nHingga ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               first                                             second\n","0  Seruling berkawan pantun,\\nTangiskan derita or...  Bandung, dasar di danau\\nLari bertumpuk di buk...\n","1  Bandung, dasar di danau\\nLari bertumpuk di buk...  Seruling menyendiri di tepi-tepi\\nTangiskan ke...\n","2  Kalau sampai waktuku\\nAku mau tak seorang kan ...  Aku ini binatang jalang\\nDari kumpulannya terb...\n","3  Aku ini binatang jalang\\nDari kumpulannya terb...  Biar peluru menembus kulitku\\nAku tetap merada...\n","4  Biar peluru menembus kulitku\\nAku tetap merada...  Luka dan bisa kubawa berlari\\nBerlari\\nHingga ..."]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"Y1I_LSeu31F1"},"source":["### Document Cleansing and Formating Train Data\n","\n","the train data would be\n","\n","prev_sentence | next_sentence"]},{"cell_type":"code","metadata":{"id":"cZVY2A0h34Pg"},"source":["import re\n","import string\n","\n","def preprocess_text(text):\n","    text = ' '.join(word.lower() for word in text.split(\" \"))\n","    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n","    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n","    return text\n","\n","def preprocess_row(row):\n","    row['first'] = preprocess_text(row['first'])\n","    row['second'] = preprocess_text(row['second'])\n","    return row"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xYSJhbQw36Uv"},"source":["poet_dataset = poet_dataset.apply(lambda row: preprocess_row(row), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"bT8YWBkVDbsm","executionInfo":{"status":"ok","timestamp":1637336545390,"user_tz":-420,"elapsed":6,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"13fd3ebe-4d1f-411c-cd7d-5dc72de164aa"},"source":["poet_dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>first</th>\n","      <th>second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>seruling berkawan pantun , tangiskan derita or...</td>\n","      <td>bandung , dasar di danau lari bertumpuk di buk...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>bandung , dasar di danau lari bertumpuk di buk...</td>\n","      <td>seruling menyendiri di tepi tepi tangiskan ker...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>kalau sampai waktuku aku mau tak seorang kan m...</td>\n","      <td>aku ini binatang jalang dari kumpulannya terbuang</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>aku ini binatang jalang dari kumpulannya terbuang</td>\n","      <td>biar peluru menembus kulitku aku tetap meradan...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>biar peluru menembus kulitku aku tetap meradan...</td>\n","      <td>luka dan bisa kubawa berlari berlari hingga hi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               first                                             second\n","0  seruling berkawan pantun , tangiskan derita or...  bandung , dasar di danau lari bertumpuk di buk...\n","1  bandung , dasar di danau lari bertumpuk di buk...  seruling menyendiri di tepi tepi tangiskan ker...\n","2  kalau sampai waktuku aku mau tak seorang kan m...  aku ini binatang jalang dari kumpulannya terbuang\n","3  aku ini binatang jalang dari kumpulannya terbuang  biar peluru menembus kulitku aku tetap meradan...\n","4  biar peluru menembus kulitku aku tetap meradan...  luka dan bisa kubawa berlari berlari hingga hi..."]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"bi4VHvFNFDIW"},"source":["# splitting data into\n","# train val and test\n","\n","poet_dataset['split'] = 'train'\n","def assign_label(row):\n","    magic_number = np.random.randint(0, 10)\n","    if magic_number > 6:\n","        valortest = np.random.randint(0, 2)\n","        if valortest == 0:\n","            return 'val'\n","        elif valortest == 1:\n","            return 'test'\n","    else:\n","        return 'train'\n","           \n","poet_dataset['split'] = poet_dataset.apply(lambda row: assign_label(row['split']), axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"xo6VsSPbFNVU","executionInfo":{"status":"ok","timestamp":1637336550669,"user_tz":-420,"elapsed":326,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"e3f4a33d-c2e9-492e-e479-4b8ad8e091f8"},"source":["poet_dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>first</th>\n","      <th>second</th>\n","      <th>split</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>seruling berkawan pantun , tangiskan derita or...</td>\n","      <td>bandung , dasar di danau lari bertumpuk di buk...</td>\n","      <td>val</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>bandung , dasar di danau lari bertumpuk di buk...</td>\n","      <td>seruling menyendiri di tepi tepi tangiskan ker...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>kalau sampai waktuku aku mau tak seorang kan m...</td>\n","      <td>aku ini binatang jalang dari kumpulannya terbuang</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>aku ini binatang jalang dari kumpulannya terbuang</td>\n","      <td>biar peluru menembus kulitku aku tetap meradan...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>biar peluru menembus kulitku aku tetap meradan...</td>\n","      <td>luka dan bisa kubawa berlari berlari hingga hi...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>128</th>\n","      <td>batapa indahnya alam kita ini ombak bergemuruh...</td>\n","      <td>kita berdiri dengan beralaskan tanah kita berd...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>129</th>\n","      <td>belum nampak mendung menutupi langit seberkas ...</td>\n","      <td>bulan tak ingin membawakan tawa manja kala wak...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>130</th>\n","      <td>kau adalah tempat yang terindah jauh dari rama...</td>\n","      <td>tempatmu yang penuh dengan pepohonan menjadika...</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>131</th>\n","      <td>saat malam tiba dengan langit yang gemerlap sa...</td>\n","      <td>namun hatiku kian murung saat awan hitam mulai...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>132</th>\n","      <td>mendadak ramainya laut angin yang menyeret deb...</td>\n","      <td>undangan dari sehamparan laut yang datang dari...</td>\n","      <td>train</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>133 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                 first  ...  split\n","0    seruling berkawan pantun , tangiskan derita or...  ...    val\n","1    bandung , dasar di danau lari bertumpuk di buk...  ...  train\n","2    kalau sampai waktuku aku mau tak seorang kan m...  ...  train\n","3    aku ini binatang jalang dari kumpulannya terbuang  ...  train\n","4    biar peluru menembus kulitku aku tetap meradan...  ...  train\n","..                                                 ...  ...    ...\n","128  batapa indahnya alam kita ini ombak bergemuruh...  ...  train\n","129  belum nampak mendung menutupi langit seberkas ...  ...  train\n","130  kau adalah tempat yang terindah jauh dari rama...  ...   test\n","131  saat malam tiba dengan langit yang gemerlap sa...  ...  train\n","132  mendadak ramainya laut angin yang menyeret deb...  ...  train\n","\n","[133 rows x 3 columns]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xxFGijGtrbPO","executionInfo":{"status":"ok","timestamp":1637336556027,"user_tz":-420,"elapsed":340,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"95c0e3c2-b4dc-4b8f-ebd3-a2c72ab44a76"},"source":["poet_dataset['split'].unique()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['val', 'train', 'test'], dtype=object)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fLT649q7dyo","executionInfo":{"status":"ok","timestamp":1637338009060,"user_tz":-420,"elapsed":643,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"1e01c1fe-010e-4001-c660-f9baa91f57ad"},"source":["poet_dataset['split'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["train    97\n","test     21\n","val      15\n","Name: split, dtype: int64"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"LJsCvpGVDwPx"},"source":["poet_dataset.to_csv(\"/content/drive/MyDrive/Pycon2021/poet_dataframe_sentences.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_0G7t-3MpYpL"},"source":["## Sequence Vocab"]},{"cell_type":"code","metadata":{"id":"oMUE2SmwpYpN"},"source":["class SequenceVocabulary(object):\n","    \"\"\"Class to extract and process vocabularies for mapping\"\"\"\n","    \n","    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n","        if token_to_idx is None:\n","            token_to_idx = {}\n","        self._token_to_idx = token_to_idx\n","        \n","        self._idx_to_token = {\n","            idx: token for token, idx in self._token_to_idx.items()\n","        }\n","        \n","        self._add_unk = add_unk\n","        self._unk_token = unk_token\n","        self._mask_token = mask_token\n","        \n","        # add begin and end sequence token\n","        self._begin_of_seq_token = \"<BEGIN-OF-SEQUENCE>\"\n","        self._end_of_seq_token = \"<END-OF-SEQUENCE>\"\n","        \n","        self.begin_seq_index = self.add_token(self._begin_of_seq_token)\n","        self.end_seq_index = self.add_token(self._end_of_seq_token)\n","\n","        self.mask_index = self.add_token(mask_token)\n","        self.unk_index = -1\n","        if add_unk:\n","            self.unk_index = self.add_token(unk_token)\n","            \n","    def to_serializeable(self):\n","        \"\"\"return a serializeable dictionary\"\"\"\n","        return {\n","            'token_to_idx': self._token_to_idx,\n","            'mask_token': self._mask_token,\n","            'add_unk': self._add_unk,\n","            'unk_token': self._unk_token\n","        }\n","    \n","    @classmethod\n","    def from_serializeable(cls, contents):\n","        \"\"\"create vocabulary object from serialize dictionary\"\"\"\n","        return cls(**contents)\n","    \n","    def add_token(self, token):\n","        \"\"\"Add a token and return it's index\"\"\"\n","        if token in self._token_to_idx:\n","            index = self._token_to_idx[token]\n","        else:\n","            index = len(self._token_to_idx)\n","            self._token_to_idx[token] = index\n","            self._idx_to_token[index] = token\n","        return index\n","    \n","    def lookup_token(self, token):\n","        \"\"\"get the index of a token \n","        if not exist returns the unk_index\"\"\"\n","        if self.unk_index >= 0:\n","            return self._token_to_idx.get(token, self.unk_index)\n","        else:\n","            return self._token_to_idx[token]\n","        \n","    def lookup_index(self, index):\n","        if index not in self._idx_to_token:\n","            raise KeyError(\"the index %d is not in the vocabulary\" % index)\n","        return self._idx_to_token[index]\n","    \n","    def __str__(self):\n","        return \"<Vocabulary(size=%d)>\" % len(self)\n","    \n","    def __len__(self):\n","        return len(self._token_to_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pj6VHBfDpYpd"},"source":["## Vectorizer"]},{"cell_type":"code","metadata":{"id":"L2AiGKcapYpf"},"source":["class NMTVectorizer(object):\n","    def __init__(self, source_vocab, target_vocab,\n","                max_source_length, max_target_length):\n","        self.source_vocab = source_vocab\n","        self.target_vocab = target_vocab\n","        self.max_source_length = max_source_length\n","        self.max_target_length = max_target_length\n","        \n","    @classmethod\n","    def from_dataframe(cls, lang_df):\n","        source_vocab = SequenceVocabulary()\n","        target_vocab = SequenceVocabulary()\n","        \n","        max_source_length, max_target_length = 0, 0\n","        \n","        for _, rows in lang_df.iterrows():\n","            # source\n","            source_token = rows[\"first\"].split(\" \")\n","            if len(source_token) > max_source_length:\n","                max_source_length = len(source_token)\n","            for token in source_token:\n","                source_vocab.add_token(token)\n","            \n","            # target\n","            target_token = rows[\"second\"].split(\" \")\n","            if len(target_token) > max_target_length:\n","                max_target_length = len(target_token)\n","            for token in target_token:\n","                target_vocab.add_token(token)\n","                \n","        return cls(source_vocab, target_vocab,\n","                  max_source_length, max_target_length)\n","    \n","    def _vectorize(self, indices, vector_length=-1, mask_index=0):\n","        if vector_length < 0:\n","            vector_length = len(indices)\n","        \n","        vector = np.zeros(vector_length, dtype=np.int64)\n","        vector[:len(indices)] = indices\n","        vector[len(indices):] = mask_index\n","        return vector\n","    \n","    def _get_source_indices(self, source_text):\n","        \"\"\"\n","        Source indices adding begin_seq_index and\n","        end_seq_index\n","        \"\"\"\n","        indices = [self.source_vocab.begin_seq_index]\n","        indices.extend(self.source_vocab.lookup_token(token) for token in\n","                       source_text.split(\" \"))\n","        indices.append(self.source_vocab.end_seq_index)\n","        \n","        return indices\n","    \n","    def _get_target_indices(self, target_text):\n","        indices = [self.target_vocab.lookup_token(token)\n","                   for token in target_text.split(\" \")]\n","        \n","        x_indices = [self.target_vocab.begin_seq_index] + indices\n","        y_indices = indices + [self.target_vocab.end_seq_index]\n","        \n","        return x_indices, y_indices\n","    \n","    def vectorize(self, source_text, target_text, use_dataset_max_length=True):\n","        source_length = -1\n","        target_length = -1\n","        \n","        if use_dataset_max_length:\n","            source_length = self.max_source_length + 2\n","            target_length = self.max_target_length + 1\n","        \n","        source_indices = self._get_source_indices(source_text)\n","        source_vector = self._vectorize(source_indices,\n","                                       source_length,\n","                                       mask_index= self.source_vocab.mask_index)\n","        \n","        target_x_indices, target_y_indices = self._get_target_indices(target_text)\n","        \n","        target_x_vector = self._vectorize(target_x_indices,\n","                                         target_length,\n","                                         self.target_vocab.mask_index)\n","        target_y_vector = self._vectorize(target_y_indices,\n","                                         target_length,\n","                                         self.target_vocab.mask_index)\n","        return {\"source_vector\": source_vector,\n","                \"target_x_vector\": target_x_vector,\n","                \"target_y_vector\": target_y_vector,\n","                \"source_length\": len(source_indices)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uc8Wgo0ApYpp"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"2bVL943wpYpr"},"source":["class NMTDataset(Dataset):\n","    def __init__(self, text_df, vectorizer):\n","        self.text_df = text_df\n","        self._vectorizer = vectorizer\n","\n","        self.train_df = self.text_df[self.text_df.split == 'train']\n","        self.train_size = len(self.train_df)\n","        \n","        self.val_df = self.text_df[self.text_df.split == 'val']\n","        self.val_size = len(self.val_df)\n","        \n","        self.test_df = self.text_df[self.text_df.split == 'test']\n","        self.test_size = len(self.test_df)\n","        \n","        self._lookup_dict = {'train': (self.train_df, self.train_size),\n","                            'val': (self.val_df, self.val_size),\n","                            'test': (self.test_df, self.test_size)}\n","        \n","        self.set_split('train')\n","        \n","    @classmethod\n","    def load_dataset_and_make_vectorizer(cls, text_csv):\n","        \"\"\"Load dataset from csv and returns the dataset object\n","        and vectorizer\"\"\"\n","        text_df = pd.read_csv(text_csv)\n","        train_text_df = text_df[text_df.split == 'train']\n","        return cls(text_df,\n","                   NMTVectorizer.from_dataframe(train_text_df))\n","    \n","    def get_vectorizer(self):\n","        \"\"\"Get vectorizer\"\"\"\n","        return self._vectorizer\n","    \n","    def set_split(self, split='train'):\n","        \"\"\"Set the split from data\"\"\"\n","        self._target_split = split\n","        self._target_df, self._target_size = self._lookup_dict[split]\n","        \n","    def __len__(self):\n","        return self._target_size\n","    \n","    def __getitem__(self, index):\n","        \"\"\"the primary entry point method for PyTorch datasets\n","        Args:\n","            index (int): the index to the data point\n","        Returns:\n","            a dict of the data point's features (x_data) and label (y_target)\n","        \"\"\"\n","        row = self._target_df.iloc[index]\n","        \n","        data_dict = self._vectorizer.vectorize(row['first'],\n","                                               row['second'])\n","        \n","        return {\n","            'x_source': data_dict[\"source_vector\"],\n","            'x_target': data_dict[\"target_x_vector\"],\n","            'y_target': data_dict[\"target_y_vector\"],\n","            'x_source_length': data_dict[\"source_length\"]\n","        }\n","    \n","    def get_num_batches(self, batch_size):\n","        \"\"\"Given the batch size return the number of batches in the dataset\"\"\"\n","        return len(self) // batch_size\n","\n","\n","def generate_nmt_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n","    \"\"\"\n","    Batch Generator\n","    \"\"\"\n","    dataloader = DataLoader(dataset, batch_size=batch_size,\n","                            shuffle=shuffle, drop_last= drop_last)\n","    \n","    for data_dict in dataloader:\n","        lengths = data_dict['x_source_length'].numpy()\n","        sorted_length_indices = lengths.argsort()[::-1].tolist()\n","        \n","        out_data_dict = {}\n","        for name, tensor in data_dict.items():\n","            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n","        \n","        yield out_data_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f7hsgwKZpYp3"},"source":["## Encoder"]},{"cell_type":"code","metadata":{"id":"p7BktL1zpYp5"},"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class NMTEncoder(nn.Module):\n","    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size):\n","        \"\"\"\n","        Encoder Module\n","        Args:\n","            num_embeddings(int): size of input dimension\n","            embedding_size(int): embedding dimension\n","            rnn_hidden_size(int): rnn hidden weight size\n","        \"\"\"\n","        super(NMTEncoder, self).__init__()\n","        \n","        self.source_embedding = nn.Embedding(num_embeddings, embedding_size, padding_idx=0)\n","        self.birnn = nn.GRU(embedding_size, rnn_hidden_size, bidirectional=True, batch_first=True)\n","        \n","    def forward(self, x_source, x_lengths):\n","        \"\"\"\n","        Forward Pass the model\n","        Args:\n","            x_source (torch.Tensor): the input data tensor.\n","                x_source.shape is (batch, seq_size)\n","            x_lengths (torch.Tensor): a vector of lengths for each item in the batch\n","        Returns:\n","            a tuple: x_unpacked (torch.Tensor), x_birnn_h (torch.Tensor)\n","                x_unpacked.shape = (batch, seq_size, rnn_hidden_size * 2)\n","                x_birnn_h.shape = (batch, rnn_hidden_size * 2)\n","        \"\"\"\n","        x_embedded = self.source_embedding(x_source)\n","        x_lengths = x_lengths.detach().cpu().numpy()\n","\n","        # create PackedSequence;\n","        # x_packed.data.shape=(number_items, embedding_size)\n","        x_packed = pack_padded_sequence(x_embedded, x_lengths, batch_first=True)\n","        \n","        # x_birnn_h.shape = (num_rnn, batch_size, feature_size)\n","        x_birnn_out, x_birnn_h = self.birnn(x_packed)\n","        # permute to (batch_size, num_rnn, feature_size)\n","        x_birnn_h = x_birnn_h.permute(1,0,2)\n","        \n","        # flatten features; reshape to (batch_size, num_rnn * feature_size)\n","        #  (recall: -1 takes the remaining positions, \n","        #           flattening the two RNN hidden vectors into 1)\n","        x_birnn_h = x_birnn_h.contiguous().view(x_birnn_h.size(0), -1)\n","        \n","        x_unpacked, _ = pad_packed_sequence(x_birnn_out, batch_first=True)\n","        return x_unpacked, x_birnn_h"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w_Rvn02fpYqF"},"source":["## Decoder"]},{"cell_type":"code","metadata":{"id":"92kP8qITpYqH"},"source":["def verbose_attention(encoder_state_vectors, query_vector):\n","    \"\"\"A descriptive version of the neural attention mechanism \n","    \n","    Args:\n","        encoder_state_vectors (torch.Tensor): 3dim tensor from bi-GRU in encoder\n","        query_vector (torch.Tensor): hidden state in decoder GRU\n","    \"\"\"\n","    batch_size, num_vectors, vector_size = encoder_state_vectors.size()\n","    \n","    vector_scores = \\\n","        torch.sum(encoder_state_vectors * query_vector.view(batch_size, 1, vector_size), dim=2)\n","    \n","    vector_probabilities = torch.softmax(vector_scores, dim=1)\n","    \n","    weighted_vectors = \\\n","        encoder_state_vectors * vector_probabilities.view(batch_size, num_vectors, 1)\n","    \n","    context_vectors = torch.sum(weighted_vectors, dim=1)\n","    return context_vectors, vector_probabilities, vector_scores\n","\n","def terse_attention(encoder_state_vectors, query_vector):\n","    \"\"\"A shorter and more optimized version of the neural attention mechanism\n","    \n","    Args:\n","        encoder_state_vectors (torch.Tensor): 3dim tensor from bi-GRU in encoder\n","        query_vector (torch.Tensor): hidden state\n","    \"\"\"\n","    vector_scores = torch.matmul(encoder_state_vectors, query_vector.unsqueeze(dim=2)).squeeze()\n","    \n","    vector_probabilities = torch.softmax(encoder_state_vectors, dim=-1)\n","    context_vectors = torch.matmul(encoder_state_vectors.transpose(-2, -1),\n","                                  vector_probabilities.unsqueeze(dim=2)).squeeze()\n","    \n","    return context_vectors, vector_probabilities\n","\n","class NMTDecoder(nn.Module):\n","    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\n","        super(NMTDecoder, self).__init__()\n","        \n","        self._rnn_hidden_size = rnn_hidden_size\n","        self.target_embedding = nn.Embedding(num_embeddings=num_embeddings,\n","                                            embedding_dim=embedding_size,\n","                                            padding_idx=0)\n","        \n","        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size, rnn_hidden_size)\n","        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n","        \n","        self.classifier = nn.Linear(rnn_hidden_size*2, num_embeddings)\n","        self.bos_index = bos_index\n","        \n","    def _init_indices(self, batch_size):\n","        \"\"\"returns the BOS index vector\"\"\"\n","        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index\n","            \n","    def _init_context_vectors(self, batch_size):\n","        \"\"\"returns a zeros vector for initializing the context\"\"\"\n","        return torch.zeros(batch_size, self._rnn_hidden_size)\n","    \n","    def forward(self, encoder_state, initial_hidden_state, target_sequence):\n","        \"\"\"The forward pass of the model\n","        \n","        Args:\n","            encoder_state (torch.Tensor): the output of the NMTEncoder\n","            initial_hidden_state (torch.Tensor): The last hidden state in the  NMTEncoder\n","            target_sequence (torch.Tensor): the target text data tensor\n","        Returns:\n","            output_vectors (torch.Tensor): prediction vectors at each output step\n","        \"\"\"\n","\n","        target_sequence = target_sequence.permute(1,0)\n","        \n","        h_t = self.hidden_map(initial_hidden_state)\n","        \n","        batch_size = encoder_state.size(0)\n","        \n","        # initialize context vector\n","        context_vectors = self._init_context_vectors(batch_size)\n","        y_t_index = self._init_indices(batch_size)\n","        \n","        h_t = h_t.to(encoder_state.device)\n","        y_t_index = y_t_index.to(encoder_state.device)\n","        context_vectors = context_vectors.to(encoder_state.device)\n","        \n","        output_vectors = []\n","        self._cached_p_attn = []\n","        self._cached_ht = []\n","        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n","        \n","        output_sequence_size = target_sequence.size(0)\n","        for i in range(output_sequence_size):\n","\n","            y_t_index = target_sequence[i]\n","            \n","            # decoding the vectors\n","            # 1. embed word and concat with previous context\n","            y_input_vector = self.target_embedding(y_t_index)\n","            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\n","            \n","            # 2. make a GRU step, getting a new hidden vector\n","            h_t = self.gru_cell(rnn_input, h_t)\n","            self._cached_ht.append(h_t.cpu().detach().numpy())\n","            \n","            # 3. use current vector to attend to encoder state\n","            context_vectors, p_attn, _ = verbose_attention(encoder_state_vectors=encoder_state,\n","                                                           query_vector = h_t)\n","            \n","            # cache the attention probabilities for visualization\n","            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\n","            \n","            # 4 use current hidden and context vectors\n","            # to make a prediction for the next word\n","            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n","            score_for_y_t_index = self.classifier(prediction_vector)\n","            \n","            # collect the prediction scores\n","            output_vectors.append(score_for_y_t_index)\n","\n","        output_vectors = torch.stack(output_vectors).permute(1, 0, 2)\n","        \n","        return output_vectors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nYUW_rnapYqN"},"source":["## NMT Model"]},{"cell_type":"code","metadata":{"id":"73udPqBNpYqO"},"source":["class NMTModel(nn.Module):\n","    def __init__(self, source_vocab_size, source_embedding_size, target_vocab_size,\n","                target_embedding_size, encoding_size, target_bos_index):\n","        \"\"\"\n","        Args:\n","            source_vocab_size (int): number of unique words in source language\n","            source_embedding_size (int): size of the source embedding vectors\n","            target_vocab_size (int): number of unique words in target language\n","            target_embedding_size (int): size of the target embedding vectors\n","            encoding_size (int): the size of the encoder RNN.  \n","        \"\"\"\n","        super(NMTModel, self).__init__()\n","        \n","        self.encoder = NMTEncoder(num_embeddings= source_vocab_size,\n","                                 embedding_size=source_embedding_size,\n","                                 rnn_hidden_size=encoding_size)\n","        \n","        decoding_size = encoding_size * 2\n","        \n","        self.decoder = NMTDecoder(num_embeddings= target_vocab_size,\n","                                 embedding_size= target_embedding_size,\n","                                 rnn_hidden_size= decoding_size,\n","                                 bos_index= target_bos_index)\n","        \n","    def forward(self, x_source, x_source_lengths, target_sequence):\n","        \"\"\"\n","        The forward pass of the model\n","        \n","        Args:\n","            x_source (torch.Tensor): the source text data tensor. \n","                x_source.shape should be (batch, vectorizer.max_source_length)\n","            x_source_lengths torch.Tensor): the length of the sequences in x_source \n","            target_sequence (torch.Tensor): the target text data tensor\n","        Returns:\n","            decoded_states (torch.Tensor): prediction vectors at each output step\n","        \"\"\"\n","        encoder_state, final_hidden_states = self.encoder(x_source,\n","                                                         x_source_lengths)\n","        decoded_states = self.decoder(encoder_state, final_hidden_states,\n","                                     target_sequence)\n","        \n","        return decoded_states"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T8f7XFJppYqV"},"source":["## Utility functions"]},{"cell_type":"code","metadata":{"id":"DTCRhwTJpYqX"},"source":["def compute_accuracy(y_pred, y_true, mask_index):\n","    y_pred, y_true = normalize_sizes(y_pred, y_true)\n","\n","    _, y_pred_indices = y_pred.max(dim=1)\n","    \n","    correct_indices = torch.eq(y_pred_indices, y_true).float()\n","    valid_indices = torch.ne(y_true, mask_index).float()\n","    \n","    n_correct = (correct_indices * valid_indices).sum().item()\n","    n_valid = valid_indices.sum().item()\n","\n","    return n_correct / n_valid * 100\n","\n","def normalize_sizes(y_pred, y_true):\n","    \"\"\"Normalize tensor sizes\n","    \n","    Args:\n","        y_pred (torch.Tensor): the output of the model\n","            If a 3-dimensional tensor, reshapes to a matrix\n","        y_true (torch.Tensor): the target predictions\n","            If a matrix, reshapes to be a vector\n","    \"\"\"\n","    if len(y_pred.size()) == 3:\n","        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n","    if len(y_true.size()) == 2:\n","        y_true = y_true.contiguous().view(-1)\n","    return y_pred, y_true\n","\n","def sequence_loss(y_pred, y_true, mask_index):\n","    y_pred, y_true = normalize_sizes(y_pred, y_true)\n","    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n","\n","\n","def update_train_state(args, model, train_state):\n","    \"\"\"\n","    Update model and early stopping\n","    \"\"\"\n","    if train_state['epoch_index'] == 0:\n","        torch.save(model.state_dict(), train_state['model_filename'])\n","\n","    # save model if performance improved\n","    elif train_state['epoch_index'] >= 1:\n","        loss_tm1, loss_t = train_state['val_loss'][-2:]\n","         \n","        # If loss worsened\n","        if loss_t >= loss_tm1:\n","            # Update step\n","            train_state['early_stopping_step'] += 1\n","        # Loss decreased\n","        else:\n","            # Save the best model\n","            if loss_t < train_state['early_stopping_best_val']:\n","                torch.save(model.state_dict(), train_state['model_filename'])\n","                train_state['early_stopping_best_val'] = loss_t\n","\n","            # Reset early stopping step\n","            train_state['early_stopping_step'] = 0\n","\n","        # Stop early ?\n","        train_state['stop_early'] = \\\n","            train_state['early_stopping_step'] >= args.early_stopping_criteria\n","\n","    return train_state"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1N_IWxtopYqg"},"source":["## Training Routine"]},{"cell_type":"code","metadata":{"id":"X_0rMHJ_pYqh"},"source":["args = Namespace(\n","    # Data information\n","    frequency_cutoff = 25,\n","    text_csv = '/content/drive/MyDrive/Pycon2021/poet_dataframe_sentences.csv',\n","    model_filename = '/content/drive/MyDrive/Pycon2021/poet_GRU_model_state.pth',\n","    # Model HyperParameters\n","    source_embedding_size=100,\n","    target_embedding_size=100,\n","    encoding_size=64,\n","    # Training HyperParameters\n","    batch_size = 10,\n","    early_stopping_criteria=5,\n","    learning_rate=0.001,\n","    momentum=0.1,\n","    num_epochs=100,\n","    seed=1337,\n","    cuda=True,\n","    dropout=0.1\n",")\n","\n","def make_train_state(args):\n","    return {'stop_early': False,\n","            'early_stopping_step': 0,\n","            'early_stopping_best_val': 1e8,\n","            'learning_rate': args.learning_rate,\n","            'epoch_index': 0,\n","            'train_loss': [],\n","            'train_acc': [],\n","            'val_loss': [],\n","            'val_acc': [],\n","            'test_loss': -1,\n","            'test_acc': -1,\n","            'model_filename': args.model_filename\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PBnz76VFpYqq","executionInfo":{"status":"ok","timestamp":1637338053176,"user_tz":-420,"elapsed":360,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"3e1883f6-4197-4f3b-aa05-971272c7890c"},"source":["train_state = make_train_state(args)\n","\n","if torch.cuda.is_available() and args.cuda:\n","  args.cuda = True\n","else:\n","  args.cuda = False\n","args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","print(\"Device available \", args.device)\n","\n","# dataset object\n","dataset = NMTDataset.load_dataset_and_make_vectorizer(args.text_csv)\n","\n","# vectorizer\n","vectorizer = dataset.get_vectorizer()\n","\n","# classifier\n","model = NMTModel(source_vocab_size= len(vectorizer.source_vocab),\n","                target_vocab_size= len(vectorizer.target_vocab),\n","                source_embedding_size = args.source_embedding_size,\n","                target_embedding_size= args.target_embedding_size,\n","                encoding_size= args.encoding_size,\n","                target_bos_index= vectorizer.target_vocab.begin_seq_index)\n","model.to(args.device)\n","\n","# optimizer\n","optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device available  cuda\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["ad0879ff197c42028aa5ade7d684f78d","3ba5b5057e3049068c87473fda3eee73","0cc37d6cf2d046a498d34379d8a83a40","9c33bb67f2564385a2f0b5cc35874495","aa1a2ab1ad9b4636b101afdffccb3331","0fc01b9a5abe4526a2cc59b6f647cf25","18516a2634d9460c93fdb8385153a8d3","c603965987ed4a5c8680f51906944ff3","419fd2dad412406aaf07ae5e66ab5040","25b9173f814e4ef394a63146686643d9","4fc991644b1b4c62bb51028334d1487e","4839d25e9ca740e5b0f412254f7ef0bf","bcb6d0f0ce4d4281bb38f9cc1a0f4476","f3592c0725a54f9e884405749bd0d3a6","3cee14fff0114e28ba9b2ffcc693af86","3da162dc2802497791dc88130b7e2db5","f37ee68cc122431da860d247b22afe84","4cda0ae7abad463c8435edec649ee6ea","70196b79ccc74515812cf4d3aa711422","c49b3577ab2845d89c1458db89c570e4","39cfd7507ef44699bd652e97c4706931","810deb7bb3c446129d03d5af257047a9","911f2b2728d640e197df88c49109930b","78c795fd57af4f03abfdee1312a0e5b5","fd5da426e2e34625a030b15ee39c33c5","02cdaae8149149c7ab9d72c582ae5f7f","73738692503343d2a6ac91cdf67ff5cb","5005edc0777a43eabfaad2dd592ec21b","8ae1531c95504611997a18ac67ff8865","702cb905c553493e81c8d8df92320d9f","516d8fb5df494995aa0a7335433b20c5","f656e8c49d1c48eea91df574bb8bdbb2","ec2caf03d9564df7bd3f5864fe785f4e"]},"id":"cxi5MEUqpYqy","executionInfo":{"status":"ok","timestamp":1637338499707,"user_tz":-420,"elapsed":439540,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"15e09186-5cc9-4e89-b830-8f8fc9e8541b"},"source":["mask_index = vectorizer.target_vocab.mask_index\n","epoch_bar = tqdm(desc='training routine', \n","                          total=args.num_epochs,\n","                          position=0)\n","\n","dataset.set_split('train')\n","train_bar = tqdm(desc='split=train',\n","                          total=dataset.get_num_batches(args.batch_size), \n","                          position=0, \n","                          leave=True)\n","dataset.set_split('val')\n","val_bar = tqdm(desc='split=val',\n","                        total=dataset.get_num_batches(args.batch_size), \n","                        position=0, \n","                        leave=True)\n","\n","try:\n","    for epoch_index in range(args.num_epochs):\n","        train_state['epoch_index'] = epoch_index\n","\n","        # Iterate over training dataset\n","\n","        # setup: batch generator, set loss and acc to 0, set train mode on\n","        dataset.set_split('train')\n","        batch_generator = generate_nmt_batches(dataset, \n","                                           batch_size=args.batch_size, \n","                                           device=args.device)\n","        running_loss = 0.0\n","        running_acc = 0.0\n","        model.train()\n","        \n","        for batch_index, batch_dict in enumerate(batch_generator):\n","            # the training routine is these 5 steps:\n","\n","            # --------------------------------------    \n","            # step 1. zero the gradients\n","            optimizer.zero_grad()\n","\n","            # step 2. compute the output\n","            y_pred = model(batch_dict['x_source'], \n","                           batch_dict['x_source_length'], \n","                           batch_dict['x_target'])\n","\n","            # step 3. compute the loss\n","            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n","\n","            # step 4. use loss to produce gradients\n","            loss.backward()\n","\n","            # step 5. use optimizer to take gradient step\n","            optimizer.step()\n","            # -----------------------------------------\n","            # compute the  running loss and running accuracy\n","            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n","            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n","            running_acc += (acc_t - running_acc) / (batch_index + 1)\n","\n","            # update bar\n","            train_bar.set_postfix(loss=running_loss,\n","                                  acc=running_acc,\n","                                  epoch=epoch_index)\n","            train_bar.update()\n","\n","        train_state['train_loss'].append(running_loss)\n","        train_state['train_acc'].append(running_acc)\n","\n","        # Iterate over val dataset\n","\n","        # setup: batch generator, set loss and acc to 0; set eval mode on\n","        dataset.set_split('val')\n","        batch_generator = generate_nmt_batches(dataset, \n","                                           batch_size=args.batch_size, \n","                                           device=args.device)\n","        running_loss = 0.\n","        running_acc = 0.\n","        model.eval()\n","\n","        for batch_index, batch_dict in enumerate(batch_generator):\n","            # compute the output\n","            y_pred = model(batch_dict['x_source'], \n","                           batch_dict['x_source_length'], \n","                           batch_dict['x_target'])\n","\n","            # step 3. compute the loss\n","            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n","\n","            # compute the  running loss and running accuracy\n","            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n","            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n","            running_acc += (acc_t - running_acc) / (batch_index + 1)\n","            \n","            # Update bar\n","            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n","                            epoch=epoch_index)\n","            val_bar.update()\n","\n","        train_state['val_loss'].append(running_loss)\n","        train_state['val_acc'].append(running_acc)\n","\n","        train_state = update_train_state(args=args, model=model, \n","                                         train_state=train_state)\n","        \n","        train_bar.n = 0\n","        val_bar.n = 0\n","        epoch_bar.update()\n","        \n","except KeyboardInterrupt:\n","    print(\"Exiting loop\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad0879ff197c42028aa5ade7d684f78d","version_minor":0,"version_major":2},"text/plain":["training routine:   0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4839d25e9ca740e5b0f412254f7ef0bf","version_minor":0,"version_major":2},"text/plain":["split=train:   0%|          | 0/9 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"911f2b2728d640e197df88c49109930b","version_minor":0,"version_major":2},"text/plain":["split=val:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"yVqhYF4iITXa"},"source":["## Evaluate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2h9CbwuHgrV","executionInfo":{"status":"ok","timestamp":1637339369530,"user_tz":-420,"elapsed":338,"user":{"displayName":"Miqdad Abdurrahman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIynZRxeNlCBrQ7O4TOkhmmUtFQJkp_Pv5nnl2iA=s64","userId":"00778005653144739378"}},"outputId":"d9d7f4db-fb1c-4cb3-cefc-988c47b38577"},"source":["def get_source_sentence(vectorizer, batch_dict, index):\n","    indices = batch_dict['x_source'][index].cpu().data.numpy()\n","    vocab = vectorizer.source_vocab\n","    return sentence_from_indices(indices, vocab)\n","\n","def get_true_sentence(vectorizer, batch_dict, index):\n","    return sentence_from_indices(batch_dict['y_target'].cpu().data.numpy()[index], vectorizer.target_vocab)\n","    \n","def get_sampled_sentence(vectorizer, batch_dict, index):\n","    y_pred = model(x_source=batch_dict['x_source'], \n","                   x_source_lengths=batch_dict['x_source_length'], \n","                   target_sequence=batch_dict['x_target'])\n","    return sentence_from_indices(torch.max(y_pred, dim=2)[1].cpu().data.numpy()[index], vectorizer.target_vocab)\n","\n","def get_all_sentences(vectorizer, batch_dict, index):\n","    return {\"source\": get_source_sentence(vectorizer, batch_dict, index), \n","            \"truth\": get_true_sentence(vectorizer, batch_dict, index), \n","            \"sampled\": get_sampled_sentence(vectorizer, batch_dict, index)}\n","    \n","def sentence_from_indices(indices, vocab, strict=True):\n","    ignore_indices = set([vocab.mask_index, vocab.begin_seq_index, vocab.end_seq_index])\n","    out = []\n","    for index in indices:\n","        if index == vocab.begin_seq_index and strict:\n","            continue\n","        elif index == vocab.end_seq_index and strict:\n","            return \" \".join(out)\n","        else:\n","            out.append(vocab.lookup_index(index))\n","    return \" \".join(out)\n","\n","\n","dataset.set_split('val')\n","batch_generator = generate_nmt_batches(dataset, \n","                                       batch_size=args.batch_size, \n","                                       device=args.device)\n","\n","test_sample = '/content/drive/MyDrive/Pycon2021/Dataset Puisi - Manual - Sheet1.csv'\n","\n","for batch_dict in batch_generator:\n","    results = get_all_sentences(vectorizer, batch_dict, 0)\n","    print(results)\n","    print('-'*100)\n","    print(\"SOURCE : \", results['source'])\n","    print(\"SAMPLED: \", results['sampled'])\n","    print(\"TRUTH: \",results['truth'])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'source': 'jam dua belas siang hari . matahari <UNK> di tengah langit . tak ada angin . tak mega . maria zaitun ke luar rumah pelacuran . tanpa <UNK> . tak ada lagi <UNK> . <UNK> <UNK> <UNK> muka . <UNK> ia berjalan . badannya <UNK> . sipilis membakar tubuhnya . penuh borok di <UNK> di leher , di <UNK> , dan di <UNK> . <UNK> merah . <UNK> kering . <UNK> <UNK> . sakit jantungnya <UNK> pula . ia pergi kepada dokter . banyak <UNK> lebih dulu menunggu . ia <UNK> di antara mereka . tiba tiba orang orang <UNK> dan menutup <UNK> mereka . ia <UNK> marah tapi <UNK> <UNK> <UNK> <UNK> . ia <UNK> <UNK> lebih dulu dan tak ada orang <UNK> . maria zaitun , <UNK> sudah banyak padaku , kata dokter . ya , <UNK> . sekarang <UNK> <UNK> ? tak ada . dokter <UNK> kepala dan <UNK> <UNK> . ia <UNK> waktu membuka <UNK> <UNK> <UNK> <UNK> di borok <UNK> . cukup , kata dokter . dan ia tak jadi <UNK> . lalu ia <UNK> kepada <UNK> <UNK> ia <UNK> vitamin c . dengan kaget <UNK> <UNK> kembali vitamin c ? dokter , paling tidak ia perlu <UNK> . untuk apa ? ia tak bisa <UNK> . dan lagi sudah jelas ia <UNK> mati . kenapa <UNK> <UNK> <UNK> <UNK> yang <UNK> dari luar <UNK> ? ', 'truth': ' malaikat penjaga firdaus . wajahnya <UNK> dan dengki dengan pedang yang menyala menuding kepadaku . aku gemetar ketakutan . hilang rasa . hilang <UNK> . maria zaitun namaku . pelacur yang takut dan <UNK> . ', 'sampled': ' malaekat penjaga firdaus wajahnya wajahnya tegas . dengki dengan pedang yang menyala menuding kepadaku . yang lesu , . maria , jijik maria , . maria zaitun namaku . pelacur yang mulya aku . . '}\n","----------------------------------------------------------------------------------------------------\n","SOURCE :  jam dua belas siang hari . matahari <UNK> di tengah langit . tak ada angin . tak mega . maria zaitun ke luar rumah pelacuran . tanpa <UNK> . tak ada lagi <UNK> . <UNK> <UNK> <UNK> muka . <UNK> ia berjalan . badannya <UNK> . sipilis membakar tubuhnya . penuh borok di <UNK> di leher , di <UNK> , dan di <UNK> . <UNK> merah . <UNK> kering . <UNK> <UNK> . sakit jantungnya <UNK> pula . ia pergi kepada dokter . banyak <UNK> lebih dulu menunggu . ia <UNK> di antara mereka . tiba tiba orang orang <UNK> dan menutup <UNK> mereka . ia <UNK> marah tapi <UNK> <UNK> <UNK> <UNK> . ia <UNK> <UNK> lebih dulu dan tak ada orang <UNK> . maria zaitun , <UNK> sudah banyak padaku , kata dokter . ya , <UNK> . sekarang <UNK> <UNK> ? tak ada . dokter <UNK> kepala dan <UNK> <UNK> . ia <UNK> waktu membuka <UNK> <UNK> <UNK> <UNK> di borok <UNK> . cukup , kata dokter . dan ia tak jadi <UNK> . lalu ia <UNK> kepada <UNK> <UNK> ia <UNK> vitamin c . dengan kaget <UNK> <UNK> kembali vitamin c ? dokter , paling tidak ia perlu <UNK> . untuk apa ? ia tak bisa <UNK> . dan lagi sudah jelas ia <UNK> mati . kenapa <UNK> <UNK> <UNK> <UNK> yang <UNK> dari luar <UNK> ? \n","SAMPLED:   malaekat penjaga firdaus wajahnya wajahnya tegas . dengki dengan pedang yang menyala menuding kepadaku . yang lesu , . maria , jijik maria , . maria zaitun namaku . pelacur yang mulya aku . . \n","TRUTH:   malaikat penjaga firdaus . wajahnya <UNK> dan dengki dengan pedang yang menyala menuding kepadaku . aku gemetar ketakutan . hilang rasa . hilang <UNK> . maria zaitun namaku . pelacur yang takut dan <UNK> . \n"]}]}]}